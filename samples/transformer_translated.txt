Die dominanten Sequenztransduktionsmodelle basieren auf komplexen rezidivierenden oder konvolutionären neuronalen Netzwerken, zu denen ein Encoder und ein Decoder gehören. Die leistungsstärksten Modelle verbinden auch den Encoder und Decoder über einen Aufmerksamkeitsmechanismus. Wir schlagen eine neue einfache Netzwerkarchitektur vor, den Transformer, der ausschließlich auf Aufmerksamkeitsmechanismen basiert, die ausschließlich auf Wiederholungs- und Konvolutionen basieren. Experimente mit zwei maschinellen Übersetzungsaufgaben zeigen, dass diese Modelle qualitativ überlegen sind, während sie parallelisierbarer sind und wesentlich weniger Zeit zum Trainieren benötigen. Unser Modell erreicht 28,4 BLEU auf der WMT 2014 Englisch-Deutsch Übersetzungsaufgabe, die sich über die bestehenden besten Ergebnisse, einschließlich Ensembles, um mehr als 2 BLEU verbessert. Auf der WMT 2014 Englisch-Französische Übersetzungsaufgabe stellt unser Modell eine neue, ein-Modell State-of-the-Art BLEU-Score von 41,8 nach einem 3,5-Tage-Training an acht GPUs, einen kleinen Bruchteil der Trainingskosten der besten Modelle aus der Literatur her. Wir zeigen, dass der Transformer sich durch erfolgreiche Anwendung auf englische Wahlkreise mit großen und begrenzten Trainingsdaten gut auf andere Aufgaben verallgemeinisiert.
Rezidivierende neuronale Netzwerke, langes Kurzzeitgedächtnis und vor allem gated rezidivierende neuronale Netzwerke wurden als state of the art Ansätze bei Sequenzmodellierungs- und Transduktionsproblemen wie Sprachmodellierung und maschineller Übersetzung fest etabliert. Zahlreiche Anstrengungen haben seitdem fortgesetzt, die Grenzen rezidivierender Sprachmodelle und Encoder-Decoder-Architekturen zu sprengen.